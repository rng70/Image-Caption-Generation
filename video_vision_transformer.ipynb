{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video vision transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNwdMGtAtcfZLTbxHBDRoLd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rng70/ViViT/blob/main/video_vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Vision Transformer\n",
        "**Original Author:** Aritra Roy Gosthipaty, Ayush Thakur <br>\n",
        "**Author:** Al Arafat Tanin\n"
      ],
      "metadata": {
        "id": "5UQG83wboTaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Videos are sequences of images. Let's assume you have an image representation model (CNN, ViT, etc.) and a sequence model (RNN, LSTM, etc.) at hand. We ask you to tweak the model for video classification. The simplest approach would be to apply the image model to individual frames, use the sequence model to learn sequences of image features, then apply a classification head on the learned sequence representation. The Keras example [Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/) explains this approach in detail. Alernatively, you can also build a hybrid Transformer-based model for video classification as shown in the Keras example [Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n",
        "\n",
        "In this example, we minimally implement [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Arnab et al., a pure Transformer-based model for video classification. The authors propose a novel embedding scheme and a number of Transformer variants to model video clips. We implement the embedding scheme and one of the variants of the Transformer architecture, for simplicity.\n",
        "\n",
        "This example requires TensorFlow 2.6 or higher, and the medmnist package, which can be installed by running the code cell below."
      ],
      "metadata": {
        "id": "cyCPHe8vqoUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq medmnist"
      ],
      "metadata": {
        "id": "No6uXeMzrdwU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ITDC99lpsuws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "c0t1IdHdst24"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "The hyperparameters are chosen via hyperparameter search. You can learn more about the process in the \"conclusion\" section."
      ],
      "metadata": {
        "id": "b-tHtOaNyESg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ],
      "metadata": {
        "id": "bhXNZTUtyFY0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "For our example we use the [MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/) dataset. The videos are lightweight and easy to train on."
      ],
      "metadata": {
        "id": "9rLPF2u6yyBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downlaod_and_prepare_dateset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "      # Get videos\n",
        "      train_videos = data['train_images']\n",
        "      valid_videos = data['val_images']\n",
        "      test_videos = data['test_images']\n",
        "\n",
        "      # Get labels\n",
        "      train_labels = data['train_labels'].flatten()\n",
        "      valid_labels = data['val_labels'].flatten()\n",
        "      test_labels = data['test_labels'].flatten()\n",
        "\n",
        "      return (\n",
        "          (train_videos, train_labels),\n",
        "          (valid_videos, valid_labels),\n",
        "          (test_videos, test_labels),\n",
        "      )\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = downlaod_and_prepare_dateset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ],
      "metadata": {
        "id": "jsBDq1rky6lg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.data pipeline"
      ],
      "metadata": {
        "id": "BVYx9DLKHLPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
      ],
      "metadata": {
        "id": "Uj_vbziFHPqQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tubelet Embedding\n",
        "In ViTs, an image is divided into patches, which are then spatially flattened, a process known as tokenization. For a video, one can repeat this process for individual frames. Uniform frame sampling as suggested by the authors is a tokenization scheme in which we sample frames from the video clip and perform simple ViT tokenization.\n",
        "$ \\frac{**uniform frame sampling**}{Uniform Frame Sampling [Source]() $"
      ],
      "metadata": {
        "id": "C8oLs8T9JNkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tkJ18mgYJut8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iDfrE8dJQE"
      },
      "source": [
        "## Tubelet Embedding\n",
        "\n",
        "In ViTs, an image is divided into patches, which are then spatially\n",
        "flattened, a process known as tokenization. For a video, one can\n",
        "repeat this process for individual frames. **Uniform frame sampling**\n",
        "as suggested by the authors is a tokenization scheme in which we\n",
        "sample frames from the video clip and perform simple ViT tokenization.\n",
        "\n",
        "| ![uniform frame sampling](https://github.com/rng70/ViViT/images/uniform%20frame%20sampling.png) |\n",
        "| :--: |\n",
        "| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n",
        "\n",
        "**Tubelet Embedding** is different in terms of capturing temporal\n",
        "information from the video.\n",
        "First, we extract volumes from the video -- these volumes contain\n",
        "patches of the frame and the temporal information as well. The volumes\n",
        "are then flattened to build video tokens.\n",
        "\n",
        "| ![tubelet embedding](https://github.com/rng70/ViViT/images/tubelet%20embedding.png) |\n",
        "| :--: |\n",
        "| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches"
      ],
      "metadata": {
        "id": "Xex_5zSrXpRQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding\n",
        "This layer adds positional information to the encoded video tokens."
      ],
      "metadata": {
        "id": "FmstZIPtXrik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens"
      ],
      "metadata": {
        "id": "JLsVMAf1X6SU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mXryImN5YFyQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aePMpb3IdJQE"
      },
      "source": [
        "## Video Vision Transformer\n",
        "\n",
        "The authors suggest 4 variants of Vision Transformer:\n",
        "\n",
        "- Spatio-temporal attention\n",
        "- Factorized encoder\n",
        "- Factorized self-attention\n",
        "- Factorized dot-product attention\n",
        "\n",
        "In this example, we will implement the **Spatio-temporal attention**\n",
        "model for simplicity. The following code snippet is heavily inspired from\n",
        "[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n",
        "One can also refer to the\n",
        "[official repository of ViViT](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit)\n",
        "which contains all the variants, implemented in JAX."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "TjEJol42YCBo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train\n"
      ],
      "metadata": {
        "id": "KULwCi6hZyxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TXWU1z_dJQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c306e89-6c23-47ce-a96a-32d74b026133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "31/31 [==============================] - 34s 504ms/step - loss: 2.4100 - accuracy: 0.1307 - top-5-accuracy: 0.6111 - val_loss: 2.2180 - val_accuracy: 0.2422 - val_top-5-accuracy: 0.7081\n",
            "Epoch 2/60\n",
            "31/31 [==============================] - 14s 444ms/step - loss: 2.0878 - accuracy: 0.2058 - top-5-accuracy: 0.7531 - val_loss: 1.9416 - val_accuracy: 0.2298 - val_top-5-accuracy: 0.7391\n",
            "Epoch 3/60\n",
            "31/31 [==============================] - 14s 444ms/step - loss: 1.9232 - accuracy: 0.2500 - top-5-accuracy: 0.8282 - val_loss: 1.7017 - val_accuracy: 0.3168 - val_top-5-accuracy: 0.8696\n",
            "Epoch 4/60\n",
            "31/31 [==============================] - 14s 465ms/step - loss: 1.7617 - accuracy: 0.2932 - top-5-accuracy: 0.8786 - val_loss: 1.5232 - val_accuracy: 0.3416 - val_top-5-accuracy: 0.9379\n",
            "Epoch 5/60\n",
            "31/31 [==============================] - 14s 464ms/step - loss: 1.6091 - accuracy: 0.3467 - top-5-accuracy: 0.9043 - val_loss: 1.4992 - val_accuracy: 0.3168 - val_top-5-accuracy: 0.9503\n",
            "Epoch 6/60\n",
            "31/31 [==============================] - 14s 460ms/step - loss: 1.4432 - accuracy: 0.4393 - top-5-accuracy: 0.9239 - val_loss: 1.2170 - val_accuracy: 0.4720 - val_top-5-accuracy: 0.9565\n",
            "Epoch 7/60\n",
            "31/31 [==============================] - 14s 461ms/step - loss: 1.3383 - accuracy: 0.4805 - top-5-accuracy: 0.9414 - val_loss: 1.1929 - val_accuracy: 0.4969 - val_top-5-accuracy: 0.9752\n",
            "Epoch 8/60\n",
            "31/31 [==============================] - 15s 494ms/step - loss: 1.3102 - accuracy: 0.4722 - top-5-accuracy: 0.9465 - val_loss: 1.0678 - val_accuracy: 0.5652 - val_top-5-accuracy: 0.9752\n",
            "Epoch 9/60\n",
            "31/31 [==============================] - 19s 596ms/step - loss: 1.2364 - accuracy: 0.5185 - top-5-accuracy: 0.9496 - val_loss: 0.9949 - val_accuracy: 0.5714 - val_top-5-accuracy: 0.9814\n",
            "Epoch 10/60\n",
            "31/31 [==============================] - 17s 558ms/step - loss: 1.2058 - accuracy: 0.5278 - top-5-accuracy: 0.9527 - val_loss: 0.8689 - val_accuracy: 0.6832 - val_top-5-accuracy: 0.9876\n",
            "Epoch 11/60\n",
            "31/31 [==============================] - 17s 538ms/step - loss: 1.0700 - accuracy: 0.5813 - top-5-accuracy: 0.9619 - val_loss: 0.7665 - val_accuracy: 0.7391 - val_top-5-accuracy: 0.9938\n",
            "Epoch 12/60\n",
            "31/31 [==============================] - 15s 468ms/step - loss: 0.9877 - accuracy: 0.6183 - top-5-accuracy: 0.9691 - val_loss: 0.7561 - val_accuracy: 0.6708 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/60\n",
            "31/31 [==============================] - 14s 446ms/step - loss: 0.9471 - accuracy: 0.6440 - top-5-accuracy: 0.9784 - val_loss: 0.8702 - val_accuracy: 0.6832 - val_top-5-accuracy: 0.9752\n",
            "Epoch 14/60\n",
            "31/31 [==============================] - 14s 464ms/step - loss: 0.9140 - accuracy: 0.6348 - top-5-accuracy: 0.9815 - val_loss: 0.7462 - val_accuracy: 0.7329 - val_top-5-accuracy: 0.9938\n",
            "Epoch 15/60\n",
            "31/31 [==============================] - 14s 443ms/step - loss: 0.8060 - accuracy: 0.6903 - top-5-accuracy: 0.9846 - val_loss: 0.6560 - val_accuracy: 0.7453 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/60\n",
            "31/31 [==============================] - 14s 441ms/step - loss: 0.7669 - accuracy: 0.7315 - top-5-accuracy: 0.9856 - val_loss: 0.4809 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
            "Epoch 17/60\n",
            "31/31 [==============================] - 13s 435ms/step - loss: 0.6272 - accuracy: 0.7840 - top-5-accuracy: 0.9897 - val_loss: 0.5235 - val_accuracy: 0.8199 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/60\n",
            "31/31 [==============================] - 13s 435ms/step - loss: 0.5904 - accuracy: 0.8076 - top-5-accuracy: 0.9938 - val_loss: 0.5101 - val_accuracy: 0.8385 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/60\n",
            "31/31 [==============================] - 14s 439ms/step - loss: 0.5864 - accuracy: 0.7778 - top-5-accuracy: 0.9959 - val_loss: 0.5114 - val_accuracy: 0.8261 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/60\n",
            "31/31 [==============================] - 15s 478ms/step - loss: 0.6273 - accuracy: 0.7819 - top-5-accuracy: 0.9846 - val_loss: 0.5055 - val_accuracy: 0.8509 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/60\n",
            "31/31 [==============================] - 13s 433ms/step - loss: 0.4806 - accuracy: 0.8374 - top-5-accuracy: 0.9918 - val_loss: 0.3943 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
            "Epoch 22/60\n",
            "31/31 [==============================] - 14s 443ms/step - loss: 0.3801 - accuracy: 0.8704 - top-5-accuracy: 0.9959 - val_loss: 0.4661 - val_accuracy: 0.8634 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/60\n",
            "31/31 [==============================] - 14s 466ms/step - loss: 0.3685 - accuracy: 0.8693 - top-5-accuracy: 0.9979 - val_loss: 0.3805 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/60\n",
            "31/31 [==============================] - 14s 457ms/step - loss: 0.3833 - accuracy: 0.8611 - top-5-accuracy: 0.9990 - val_loss: 0.3610 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
            "Epoch 25/60\n",
            "31/31 [==============================] - 14s 461ms/step - loss: 0.3050 - accuracy: 0.8981 - top-5-accuracy: 0.9990 - val_loss: 0.4150 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
            "Epoch 26/60\n",
            "31/31 [==============================] - 14s 449ms/step - loss: 0.2785 - accuracy: 0.9023 - top-5-accuracy: 0.9979 - val_loss: 0.3222 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/60\n",
            "31/31 [==============================] - 14s 465ms/step - loss: 0.2503 - accuracy: 0.9105 - top-5-accuracy: 0.9990 - val_loss: 0.2723 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/60\n",
            "31/31 [==============================] - 14s 442ms/step - loss: 0.2369 - accuracy: 0.9228 - top-5-accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/60\n",
            "31/31 [==============================] - 14s 443ms/step - loss: 0.2281 - accuracy: 0.9249 - top-5-accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/60\n",
            "31/31 [==============================] - 14s 441ms/step - loss: 0.1884 - accuracy: 0.9331 - top-5-accuracy: 1.0000 - val_loss: 0.2705 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
            "Epoch 31/60\n",
            "31/31 [==============================] - 14s 459ms/step - loss: 0.1570 - accuracy: 0.9568 - top-5-accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
            "Epoch 32/60\n",
            "31/31 [==============================] - 14s 464ms/step - loss: 0.1644 - accuracy: 0.9506 - top-5-accuracy: 1.0000 - val_loss: 0.3684 - val_accuracy: 0.8758 - val_top-5-accuracy: 1.0000\n",
            "Epoch 33/60\n",
            "31/31 [==============================] - 14s 464ms/step - loss: 0.1391 - accuracy: 0.9496 - top-5-accuracy: 1.0000 - val_loss: 0.3338 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
            "Epoch 34/60\n",
            "31/31 [==============================] - 14s 441ms/step - loss: 0.0992 - accuracy: 0.9753 - top-5-accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
            "Epoch 35/60\n",
            "31/31 [==============================] - 15s 482ms/step - loss: 0.1141 - accuracy: 0.9599 - top-5-accuracy: 1.0000 - val_loss: 0.4539 - val_accuracy: 0.8820 - val_top-5-accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "fWW9zkojak3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES_VIZ = 25\n",
        "testsamples, labels = next(iter(testloader))\n",
        "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
        "\n",
        "ground_truths = []\n",
        "preds = []\n",
        "videos = []\n",
        "\n",
        "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
        "    # Generate gif\n",
        "    with io.BytesIO() as gif:\n",
        "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
        "        videos.append(gif.getvalue())\n",
        "\n",
        "    # Get model prediction\n",
        "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
        "    pred = np.argmax(output, axis=0)\n",
        "\n",
        "    ground_truths.append(label.numpy().astype(\"int\"))\n",
        "    preds.append(pred)\n",
        "\n",
        "\n",
        "def make_box_for_grid(image_widget, fit):\n",
        "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
        "\n",
        "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
        "    \"\"\"\n",
        "    # Make the caption\n",
        "    if fit is not None:\n",
        "        fit_str = \"'{}'\".format(fit)\n",
        "    else:\n",
        "        fit_str = str(fit)\n",
        "\n",
        "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
        "\n",
        "    # Make the green box with the image widget inside it\n",
        "    boxb = ipywidgets.widgets.Box()\n",
        "    boxb.children = [image_widget]\n",
        "\n",
        "    # Compose into a vertical box\n",
        "    vb = ipywidgets.widgets.VBox()\n",
        "    vb.layout.align_items = \"center\"\n",
        "    vb.children = [h, boxb]\n",
        "    return vb\n",
        "\n",
        "\n",
        "boxes = []\n",
        "for i in range(NUM_SAMPLES_VIZ):\n",
        "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
        "    true_class = info[\"label\"][str(ground_truths[i])]\n",
        "    pred_class = info[\"label\"][str(preds[i])]\n",
        "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
        "\n",
        "    boxes.append(make_box_for_grid(ib, caption))\n",
        "\n",
        "ipywidgets.widgets.GridBox(\n",
        "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
        ")"
      ],
      "metadata": {
        "id": "sve7IbP3aoVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMvacSiudJQE"
      },
      "source": [
        "## Final thoughts\n",
        "\n",
        "With a vanilla implementation, we achieve ~79-80% Top-1 accuracy on the\n",
        "test dataset.\n",
        "\n",
        "The hyperparameters used in this tutorial were finalized by running a\n",
        "hyperparameter search using\n",
        "[W&B Sweeps](https://docs.wandb.ai/guides/sweeps).\n",
        "You can find out our sweeps result\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/sweeps/66fp0lhz)\n",
        "and our quick analysis of the results\n",
        "[here](https://wandb.ai/minimal-implementations/vivit/reports/Hyperparameter-Tuning-Analysis--VmlldzoxNDEwNzcx).\n",
        "\n",
        "For further improvement, you could look into the following:\n",
        "\n",
        "- Using data augmentation for videos.\n",
        "- Using a better regularization scheme for training.\n",
        "- Apply different variants of the transformer model as in the paper.\n",
        "\n",
        "We would like to thank [Anurag Arnab](https://anuragarnab.github.io/)\n",
        "(first author of ViViT) for helpful discussion. We are grateful to\n",
        "[Weights and Biases](https://wandb.ai/site) program for helping with\n",
        "GPU credits.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/video-vision-transformer) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/video-vision-transformer-CT)."
      ]
    }
  ]
}