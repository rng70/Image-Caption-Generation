{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video vision transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVLcBGi3I7JDeRxN88/74C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rng70/ViViT/blob/main/video_vision_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Vision Transformer\n",
        "**Original Author:** Aritra Roy Gosthipaty, Ayush Thakur <br>\n",
        "**Author:** Al Arafat Tanin\n"
      ],
      "metadata": {
        "id": "5UQG83wboTaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Videos are sequences of images. Let's assume you have an image representation model (CNN, ViT, etc.) and a sequence model (RNN, LSTM, etc.) at hand. We ask you to tweak the model for video classification. The simplest approach would be to apply the image model to individual frames, use the sequence model to learn sequences of image features, then apply a classification head on the learned sequence representation. The Keras example [Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/) explains this approach in detail. Alernatively, you can also build a hybrid Transformer-based model for video classification as shown in the Keras example [Video Classification with Transformers](https://keras.io/examples/vision/video_transformers/).\n",
        "\n",
        "In this example, we minimally implement [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Arnab et al., a pure Transformer-based model for video classification. The authors propose a novel embedding scheme and a number of Transformer variants to model video clips. We implement the embedding scheme and one of the variants of the Transformer architecture, for simplicity.\n",
        "\n",
        "This example requires TensorFlow 2.6 or higher, and the medmnist package, which can be installed by running the code cell below."
      ],
      "metadata": {
        "id": "cyCPHe8vqoUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq medmnist"
      ],
      "metadata": {
        "id": "No6uXeMzrdwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ITDC99lpsuws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "c0t1IdHdst24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "The hyperparameters are chosen via hyperparameter search. You can learn more about the process in the \"conclusion\" section."
      ],
      "metadata": {
        "id": "b-tHtOaNyESg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ],
      "metadata": {
        "id": "bhXNZTUtyFY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "For our example we use the [MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/) dataset. The videos are lightweight and easy to train on."
      ],
      "metadata": {
        "id": "9rLPF2u6yyBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def downlaod_and_prepare_dateset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "      # Get videos\n",
        "      train_videos = data['train_images']\n",
        "      valid_videos = data['val_images']\n",
        "      test_videos = data['test_images']\n",
        "\n",
        "      # Get labels\n",
        "      train_labels = data['train_labels'].flatten()\n",
        "      valid_labels = data['val_labels'].flatten()\n",
        "      test_labels = data['test_labels'].flatten()\n",
        "\n",
        "      return (\n",
        "          (train_videos, train_labels),\n",
        "          (valid_videos, valid_labels),\n",
        "          (test_videos, test_labels),\n",
        "      )\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = downlaod_and_prepare_dateset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ],
      "metadata": {
        "id": "jsBDq1rky6lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.data pipeline"
      ],
      "metadata": {
        "id": "BVYx9DLKHLPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
      ],
      "metadata": {
        "id": "Uj_vbziFHPqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tubelet Embedding\n",
        "In ViTs, an image is divided into patches, which are then spatially flattened, a process known as tokenization. For a video, one can repeat this process for individual frames. Uniform frame sampling as suggested by the authors is a tokenization scheme in which we sample frames from the video clip and perform simple ViT tokenization.\n",
        "$ \\frac{**uniform frame sampling**}{Uniform Frame Sampling [Source]() $"
      ],
      "metadata": {
        "id": "C8oLs8T9JNkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tkJ18mgYJut8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39iDfrE8dJQE"
      },
      "source": [
        "## Tubelet Embedding\n",
        "\n",
        "In ViTs, an image is divided into patches, which are then spatially\n",
        "flattened, a process known as tokenization. For a video, one can\n",
        "repeat this process for individual frames. **Uniform frame sampling**\n",
        "as suggested by the authors is a tokenization scheme in which we\n",
        "sample frames from the video clip and perform simple ViT tokenization.\n",
        "\n",
        "| ![uniform frame sampling](https://github.com/rng70/ViViT/blob/main/images/uniform%20frame%20sampling.png) |\n",
        "| :--: |\n",
        "| Uniform Frame Sampling [Source](https://arxiv.org/abs/2103.15691) |\n",
        "\n",
        "**Tubelet Embedding** is different in terms of capturing temporal\n",
        "information from the video.\n",
        "First, we extract volumes from the video -- these volumes contain\n",
        "patches of the frame and the temporal information as well. The volumes\n",
        "are then flattened to build video tokens.\n",
        "\n",
        "| ![tubelet embedding](https://github.com/rng70/ViViT/blob/main/images/tubelet%20embedding.png) |\n",
        "| :--: |\n",
        "| Tubelet Embedding [Source](https://arxiv.org/abs/2103.15691) |"
      ]
    }
  ]
}